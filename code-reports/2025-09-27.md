### Current state of the project
* **Static Vite-powered frontend.** The web UI renders a catalogue of 2,000+ starred repositories from a pre-generated `data.json`. It includes filtering (language, tag, license, sort), animated counters, card style toggles, README preview panels, and localStorage-backed activity logging from the fixed layout in `index.html` and the event-rich controller in `main.js`.

* **Markdown archive.** The root `README.md` is a massive “awesome list” style export listing every starred repository grouped by language, mirroring what the app shows and indicating the automation goal.

* **Data generation workflow.** `scripts/generator.js` paginates the GitHub API for the `KBLLR` account, pulls language breakdowns for each repo, and transforms the results into language buckets that power both the frontend JSON and the README snapshot.

* **Streamlit companion app.** `src/streamlit_app/app.py` and `utils.py` build an exploratory dashboard (cards, network graph, project generator) from the same `data.json`, synthesizing “modules” and “papers” from repository topics to demonstrate richer analytics around the starred data.



### Notable gaps & friction points
* **Missing DOM target for log badge.** The frontend code updates `logsCountBadge`, but the markup only defines `logCount`, so half the badge updates silently fail, leaving the header counter stuck at “0” after interactions.

* **Heavy, unthrottled GitHub API usage.** `transformData` fires `getRepoLanguages` for every starred repository inside a single `Promise.all`, which can translate to thousands of concurrent language requests and easily blow through GitHub’s secondary rate limits.

* **Synthetic analytics can feel inconsistent.** The Streamlit utilities fabricate university modules and research papers by sampling repo topics; this works for demos but makes it hard to trust or reproduce the insights in production.

* **No automated checks.** `npm test` is stubbed to exit with an error, so there’s no guardrail verifying data generation, bundle health, or linting before publishing.



### Suggested improvements
1. **Align DOM hooks with script expectations.** Add an element with `id="logsCountBadge"` (or switch the JavaScript to reuse `logCount`) so the log counter reflects reality both on the main page and in the dedicated logs view.

2. **Throttle GitHub enrichment requests.** Wrap `getRepoLanguages` calls in a queue (for example, use the existing `p-queue` dependency) or batch them to respect GitHub’s rate limits, and cache language data to avoid re-fetching unchanged repositories between runs.

3. **Surface log history more clearly.** Persist the log table into a JSON export or a pinned section in the README so interactions don’t live exclusively in localStorage, and add filters for action types directly in the main app to reduce navigation friction.

4. **Ground analytics in real data.** Replace Streamlit’s synthetic modules/papers with metadata derived from tagged repositories or external sources (e.g., GitHub topics, README keywords) to make the dashboard actionable rather than illustrative.

5. **Automate quality checks.** Introduce linting (ESLint) and a lightweight unit/integration test that ensures `data.json` loads, filters apply, and logs persist. Update the `npm test` script to run those checks so CI can catch regressions before deployment.

