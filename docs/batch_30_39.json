[
  {
    "title": "3D-aware Image Generation using 2D Diffusion Models",
    "description": "This research introduces a new method for 3D-aware image generation that leverages the power of 2D diffusion models. The method redefines the task as a sequential process of generating multiple views of an image, using both unconditional and conditional diffusion models. This approach allows for the generation of high-quality, multi-view images from large-scale datasets like ImageNet, as well as smaller, more specialized datasets. The method's effectiveness is demonstrated through comparisons with existing 3D-aware GANs and through the generation of images with large view angles, even up to 360 degrees.",
    "tags": [
      "3D-aware Image Generation",
      "2D Diffusion Models",
      "Multiview Image Generation",
      "Image Synthesis",
      "Generative Modeling"
    ],
    "document": "documents/2303.17905v1.pdf"
  },
  {
    "title": "Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing",
    "description": "The document presents a novel framework for editing fashion images based on multimodal inputs such as text descriptions, human poses, and garment sketches. The authors introduce a new architecture called Multimodal Garment Designer (MGD) that builds upon latent diffusion models and incorporates these multimodal conditions to guide the image generation process. To address the lack of suitable datasets, the authors extend existing fashion datasets (Dress Code and VITON-HD) with textual descriptions and garment sketches, providing a valuable resource for future research in this area. Experimental results demonstrate the effectiveness of the proposed approach in generating high-quality, realistic images that adhere to the given multimodal inputs, outperforming existing methods and baselines.",
    "tags": [
      "Fashion Image Editing",
      "Latent Diffusion Models",
      "Multimodal Conditioning",
      "Garment Sketches",
      "Human Pose Estimation"
    ],
    "document": "documents/2304.02051v2.pdf"
  },
  {
    "title": "Differential Diffusion: Giving Each Pixel Its Strength",
    "description": "This paper introduces a novel framework that allows users to change an arbitrary number of regions of an image by different strengths efficiently and simultaneously. This framework can be integrated into any existing diffusion model, enhancing it with the capability to customize the amount of change per pixel or per image region. Such granular control on the quantity of change opens up a diverse array of new editing capabilities, such as control of the extent to which individual objects are modified, or the ability to introduce gradual spatial changes.",
    "tags": [
      "Diffusion Models",
      "Image Editing",
      "Change Map",
      "Soft-Inpainting",
      "Strength Fan"
    ],
    "document": "documents/2306.00950v2.pdf"
  },
  {
    "title": "Restart Sampling for Improving Generative Processes",
    "description": "This document proposes a novel sampling algorithm called \"Restart\" to improve the speed and quality of generative processes that involve solving differential equations, such as diffusion models. The algorithm addresses the limitations of existing ODE and SDE samplers by combining their strengths. It alternates between adding substantial noise (akin to restarting the process) and strictly following a backward ODE, thereby reducing discretization errors while leveraging the contraction effect of stochasticity. Experiments demonstrate that Restart surpasses previous SDE and ODE samplers in both quality and speed across various datasets and pre-trained models, including text-to-image generation with Stable Diffusion.",
    "tags": [
      "Generative Processes",
      "Diffusion Models",
      "Sampling Algorithms",
      "Differential Equations",
      "ODE",
      "SDE",
      "Restart Sampling"
    ],
    "document": "documents/2306.14878v2.pdf"
  },
  {
    "title": "BOOSTING END-TO-END MULTILINGUAL PHONEME RECOGNITION THROUGH\nEXPLOITING UNIVERSAL SPEECH ATTRIBUTES CONSTRAINTS",
    "description": "The document proposes a novel approach to multilingual end-to-end automatic speech recognition (ASR) by integrating knowledge about speech articulators. The key idea is to leverage a rich set of fundamental units that can be defined \u201cuniversally\u201d across all spoken languages, referred to as speech attributes, namely manner and place of articulation. The proposed solution outperforms conventional multilingual approaches with a relative improvement of 6.85% on average, and it also demonstrates a much better performance compared to the monolingual model. Further analysis conclusively demonstrates that the proposed solution eliminates phoneme predictions that are inconsistent with attributes.",
    "tags": [
      "Multilingual Automatic Speech Recognition",
      "Articulatory Speech Attributes",
      "Phoneme Recognition",
      "Attribute-to-Phoneme Mapping",
      "Knowledge-Based Constraints"
    ],
    "document": "documents/2309.08828v1.pdf"
  },
  {
    "title": "Perceptual Structure in the Absence of Grounding for LLMs: The Impact of Abstractedness and Subjectivity in Color Language",
    "description": "The document explores the ability of Large Language Models (LLMs) to understand color language, particularly focusing on how they handle abstract and subjective color descriptions. The authors conducted two experiments. The first assessed the alignment between color spaces and the feature spaces of LLMs, finding that alignment decreases as the subjectivity of color descriptions increases. The second experiment evaluated the ability of LLMs to determine the correct comparative relationship between color descriptions (e.g., darker or lighter) without access to color space information. Surprisingly, LLMs performed well on this task, even with highly subjective descriptions. The findings suggest that while LLMs can learn some degree of perceptual structure from language alone, they might still require grounding in color spaces for tasks involving complex or subjective color descriptions.",
    "tags": [
      "Large Language Models (LLMs)",
      "Color Language",
      "Grounding",
      "Perceptual Structure",
      "Abstractedness and Subjectivity"
    ],
    "document": "documents/2311.13105v1.pdf"
  },
  {
    "document": "documents/2401.02524v2.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "REAL3D-PORTRAIT: ONE-SHOT REALISTIC 3D\nTALKING PORTRAIT SYNTHESIS",
    "description": "This document presents Real3D-Portrait, a novel framework for synthesizing one-shot realistic 3D talking portraits from a single image and audio or video. The authors address limitations in previous methods, such as inaccurate reconstruction and animation, by introducing a pre-trained image-to-plane model and a motion adapter. The framework also incorporates a head-torso-background paradigm for realistic torso movement and switchable backgrounds, and a generic audio-to-motion model for audio-driven applications. Experiments demonstrate superior performance over existing one-shot methods in identity preservation, visual quality, and audio-lip synchronization.",
    "tags": [
      "3D Talking Portrait Synthesis",
      "One-Shot Learning",
      "Image-to-Plane Model",
      "Motion Adapter",
      "Audio-to-Motion Model"
    ],
    "document": "documents/2401.08503v3.pdf"
  },
  {
    "title": "HeadStudio: Text to Animatable Head Avatars\nwith 3D Gaussian Splatting",
    "description": "The paper introduces HeadStudio, a novel framework that utilizes 3D Gaussian splatting to generate realistic and animatable avatars from text prompts. The framework addresses the limitations of previous methods that struggle to combine high-quality and animation effectively in text-based avatar generation. HeadStudio associates 3D Gaussians with an animatable head prior model, facilitating semantic animation on high-quality 3D representations. It enhances optimization from initialization, distillation, and regularization to jointly learn the shape, texture, and animation, ensuring consistent and appealing animation. Experiments demonstrate HeadStudio's efficacy in generating animatable avatars from textual prompts, exhibiting high-quality real-time rendering and smooth driving by real-world speech and video.",
    "tags": [
      "Head avatar animation",
      "Text-guided generation",
      "3D Gaussian splatting",
      "Animatable Head Gaussian",
      "Text to Avatar Optimization"
    ],
    "document": "documents/2402.06149v2.pdf"
  },
  {
    "document": "documents/2403.01779v2.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  }
]