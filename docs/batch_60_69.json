[
  {
    "title": "Structured 3D Latents for Scalable and Versatile 3D Generation",
    "description": "This document presents a novel 3D generation method called TRELLIS, which utilizes a unified Structured LATent (SLAT) representation to enable versatile and high-quality 3D asset creation. SLAT combines sparse 3D structures with powerful visual representations, allowing for decoding into various output formats like Radiance Fields, 3D Gaussians, and meshes. The method employs rectified flow transformers tailored for SLAT generation and trains models with up to 2 billion parameters on a large 3D asset dataset. TRELLIS surpasses existing methods in generating high-quality 3D assets with detailed geometry and vivid textures, offering flexible output format selection and local 3D editing capabilities.",
    "tags": [
      "3D Generation",
      "Structured Latent Representation (SLAT)",
      "Rectified Flow Transformers",
      "Versatile 3D Asset Creation",
      "3D Editing"
    ],
    "document": "documents/2412.01506v1.pdf"
  },
  {
    "title": "Motion Prompting: Controlling Video Generation with Motion Trajectories",
    "description": "This research paper introduces a novel framework called \"motion prompting\" for controlling video generation using motion trajectories. The authors argue that traditional text-based prompts struggle to capture the nuances of motion, and propose using spatio-temporally sparse or dense motion trajectories as a more expressive and flexible alternative. They train a ControlNet model on a dataset of videos paired with motion tracks, enabling the model to generate videos conditioned on these motion prompts. The authors demonstrate the versatility of their approach through various applications, including object and camera motion control, motion transfer, and drag-based image editing. They also highlight the emergence of realistic physics in the generated videos, suggesting the potential of motion prompts for probing video models and interacting with future generative world models. Quantitative and human evaluations demonstrate the effectiveness of their method compared to recent baselines.",
    "tags": [
      "Video Generation",
      "Motion Control",
      "Motion Trajectories",
      "ControlNet",
      "Motion Prompt Expansion"
    ],
    "document": "documents/2412.02700v1.pdf"
  },
  {
    "title": "Align3R: Aligned Monocular Depth Estimation for Dynamic Videos",
    "description": "Estimating temporally consistent depth for dynamic videos from monocular videos is a challenging task.  Existing methods either rely on computationally expensive optimization during inference or struggle to generalize to complex real-world dynamics. This paper introduces Align3R, a novel approach that leverages the strengths of monocular depth estimators and the DUSt3R model to achieve both accurate and temporally consistent video depth estimation. Align3R incorporates estimated monocular depth information during the fine-tuning of DUSt3R, enabling it to predict detailed pairwise point maps for dynamic scenes. By enforcing geometric constraints through global alignment, Align3R produces accurate depth maps and camera poses for dynamic videos, outperforming existing methods on various benchmark datasets.",
    "tags": [
      "monocular depth estimation",
      "dynamic videos",
      "temporal consistency",
      "DUSt3R",
      "camera pose estimation"
    ],
    "document": "documents/2412.03079v2.pdf"
  },
  {
    "title": "INFP: Audio-Driven Interactive Head Generation in Dyadic Conversations",
    "description": "The document presents INFP, a novel audio-driven head generation framework for dyadic interaction. Unlike previous works that focus on single-sided communication or require manual role assignment, INFP allows the agent to dynamically alternate between speaking and listening states based on the input audio. The framework comprises a Motion-Based Head Imitation stage and an Audio-Guided Motion Generation stage. The first stage learns to project facial communicative behaviors from real-life conversation videos into a low-dimensional motion latent space and uses these codes to animate a static image. The second stage learns the mapping from input dyadic audio to motion latent codes through denoising, leading to audio-driven head generation in interactive scenarios. The authors also introduce DyConv, a large-scale dataset of rich dyadic conversations collected from the Internet, to facilitate this line of research. Extensive experiments and visualizations demonstrate the superior performance and effectiveness of their method.",
    "tags": [
      "Head Generation",
      "Dyadic Interaction",
      "Audio-Driven Animation",
      "Motion Latent Space",
      "Conversational Agent"
    ],
    "document": "documents/2412.04037v1.pdf"
  },
  {
    "document": "documents/2412.07589v1.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "StyleMaster: Stylize Your Video with Artistic Generation and Translation",
    "description": "This research paper introduces StyleMaster, a novel approach for video stylization that addresses the limitations of existing methods in terms of sub-optimal style extraction and the lack of video translation capabilities. The authors propose a novel style extraction module that leverages both global and local style representations. To enhance video quality, they incorporate a motion adapter, which improves motion quality and style extent during inference. Additionally, they implement a gray tile ControlNet for more precise content guidance in video translation tasks. The paper presents experimental results demonstrating that StyleMaster significantly outperforms other methods in both text alignment and style resemblance.",
    "tags": [
      "Video Stylization",
      "Style Transfer",
      "Text-to-Video Generation",
      "Motion Adapter",
      "ControlNet"
    ],
    "document": "documents/2412.07744v1.pdf"
  },
  {
    "title": "Representing Long Volumetric Video with Temporal Gaussian Hierarchy",
    "description": "This paper introduces a novel 4D representation called Temporal Gaussian Hierarchy for efficiently modeling long volumetric videos. The approach addresses the limitations of previous methods that struggle with the high computational and memory requirements of long video sequences. By leveraging a hierarchical structure with varying temporal scales, the method efficiently represents dynamic scenes with varying degrees of motion. Additionally, a Compact Appearance Model is proposed to reduce storage overhead while maintaining rendering quality. The paper demonstrates state-of-the-art results on various datasets, showcasing the method's ability to handle long videos with high fidelity and real-time rendering capabilities.",
    "tags": [
      "Novel view synthesis",
      "Neural rendering",
      "Neural radiance field",
      "3D Gaussian splatting",
      "4D Gaussian Splatting"
    ],
    "document": "documents/2412.09608v1.pdf"
  },
  {
    "title": "ColorFlow: Retrieval-Augmented Image Sequence Colorization",
    "description": "This document introduces ColorFlow, a novel three-stage diffusion-based framework for colorizing black-and-white image sequences using a pool of reference images. The method addresses the challenge of preserving character and object identity across frames, a crucial aspect overlooked by previous approaches. ColorFlow leverages a Retrieval-Augmented Pipeline to extract relevant color information from reference images, an In-context Colorization Pipeline with a two-branch design for accurate colorization, and a Guided Super-Resolution Pipeline to enhance output quality.  Evaluations on a newly introduced benchmark dataset, ColorFlow-Bench, demonstrate ColorFlow's superior performance over existing methods in both pixel-wise and image-wise metrics, achieving finer-grained color identity preservation and significant image quality improvements.",
    "tags": [
      "Image Colorization",
      "Diffusion Models",
      "Retrieval-Augmented Generation",
      "Image Sequence Colorization",
      "Identity Preservation"
    ],
    "document": "documents/2412.11815v1.pdf"
  },
  {
    "title": "Wonderland: Navigating 3D Scenes from a Single Image",
    "description": "The document presents Wonderland, a novel framework for efficiently generating high-quality 3D scenes from a single image. It leverages the generative capabilities of video diffusion models to overcome limitations of previous methods that rely on per-scene optimization or struggle with multi-view consistency. Wonderland introduces a dual-branch camera conditioning mechanism for precise pose control and a latent-based large reconstruction model (LaLRM) for efficient 3D scene synthesis. Evaluations demonstrate its superior performance in generating high-fidelity and geometrically consistent 3D scenes from single views, outperforming existing methods in both video generalization and 3D rendering quality.",
    "tags": [
      "\"3D Scene Generation\"",
      "\"Single-View Reconstruction\"",
      "\"Video Diffusion Models\"",
      "\"Camera Control\"",
      "\"Gaussian Splatting\""
    ],
    "document": "documents/2412.12091v1.pdf"
  },
  {
    "document": "documents/2412.12093v1.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  }
]