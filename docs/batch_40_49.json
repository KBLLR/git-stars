[
  {
    "title": "MOTION INVERSION FOR VIDEO CUSTOMIZATION",
    "description": "The document presents a novel approach for customizing motion in video generation using Motion Embeddings. This method addresses the challenge of representing motion in generative models by introducing two types of embeddings: Motion Query-Key Embedding and Motion Value Embedding. These embeddings are integrated into temporal transformer modules of video diffusion models, enabling the manipulation of inter-frame dynamics. The method excels at preserving motion trajectories and object poses from source videos while aligning generated visuals with textual descriptions. Experiments demonstrate its superior performance over existing motion customization techniques in terms of motion fidelity, text similarity, and user preference.",
    "tags": [
      "Motion Customization",
      "Video Generation",
      "Motion Embeddings",
      "Temporal Transformer",
      "Diffusion Models"
    ]
  },
  {
    "error": "400 Request contains an invalid argument.",
    "document": "documents/2404.02733v2.pdf",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "Magic Clothing: Controllable Garment-Driven Image Synthesis",
    "description": "The document presents Magic Clothing, a novel LDM-based network architecture for garment-driven image synthesis. The authors introduce a garment extractor to capture detailed garment features and incorporate them into pretrained LDMs through self-attention fusion. This approach ensures the preservation of garment details while maintaining faithfulness to text prompts. The method allows for generating customized characters wearing target garments based on diverse text prompts.  The authors also propose a robust metric called Matched-Points-LPIPS (MP-LPIPS) to evaluate the consistency between the generated image and the source garment. Extensive experiments demonstrate the superiority of Magic Clothing in generating diverse and controllable images for garment-driven image synthesis.",
    "tags": [
      "Garment-Driven Image Synthesis",
      "Latent Diffusion Models",
      "Controllable Image Generation",
      "Virtual Try-On",
      "Subject-Driven Image Synthesis"
    ]
  },
  {
    "title": "EMOPortraits: Emotion-enhanced Multimodal One-shot Head Avatars",
    "description": "This document presents EMOPortraits, a novel method for generating realistic, one-shot head avatars that excel in transferring intense and asymmetric facial expressions. The authors highlight the limitations of existing methods in conveying the nuances of strong facial expressions, particularly in cross-driving synthesis. They address these limitations by introducing substantial changes to both the training pipeline and model architecture, focusing on the development of a robust latent expression space and the integration of a novel dataset featuring a wide range of extreme facial expressions. Additionally, the authors incorporate a speech-driven mode, enabling the animation of source identity through diverse modalities, including visual signals, audio, or a blend of both.",
    "tags": [
      "Head Avatars",
      "Facial Expression Transfer",
      "One-Shot Learning",
      "Speech-Driven Animation",
      "Cross-Driving Synthesis"
    ]
  },
  {
    "title": "Diffusion for World Modeling: Visual Details Matter in Atari",
    "description": "The document introduces DIAMOND, a new reinforcement learning agent trained within a diffusion world model. The authors argue that existing world models, which often rely on compressing visual information into discrete latent variables, may lose important visual details crucial for effective reinforcement learning. DIAMOND addresses this by directly modeling the environment dynamics in the image space using a diffusion model, thus preserving more visual details. The authors demonstrate the effectiveness of DIAMOND on the Atari 100k benchmark, achieving state-of-the-art performance for agents trained entirely within a world model. They also provide analysis and comparisons with other world model baselines, highlighting the importance of visual fidelity for improved performance.",
    "tags": [
      "World Models",
      "Reinforcement Learning",
      "Diffusion Models",
      "Atari 100k Benchmark",
      "Visual Details"
    ]
  },
  {
    "error": "Exceeded retry limit",
    "document": "documents/2406.16260v1.pdf",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "Graph-Based Captioning: Enhancing Visual Descriptions by Interconnecting Region Captions",
    "description": "This paper proposes a new annotation strategy called graph-based captioning (GBC) that describes an image using a labelled graph structure. GBC contains four types of nodes: an image node with captions of the entire image, entity nodes that contain descriptions of individual objects, composition nodes that link objects in the images of the same type, and relation nodes that describe the spatial or semantic relationships between objects of different types. The authors demonstrate that GBC can be produced automatically using off-the-shelf multimodal LLMs and open-vocabulary detection models and create a new dataset, GBC10M, gathering GBC annotations for about 10M images of the CC12M dataset. They show that using GBC nodes' annotations results in significant performance boost on downstream models when compared to other dataset formats.",
    "tags": [
      "Graph-Based Captioning",
      "Vision-Language",
      "Image Captioning",
      "Compositionality",
      "Scene Graphs"
    ]
  },
  {
    "title": "Neural Localizer Fields for Continuous\n3D Human Pose and Shape Estimation",
    "description": "The authors propose a new method for 3D human pose and shape estimation from a single RGB image. The method, called Neural Localizer Field (NLF), learns a continuous field of point localizers that can predict the 3D location of any point on the human body. This allows the model to be trained on datasets with different annotation formats, such as meshes, 3D skeletons, 2D keypoints, and DensePose, without the need for re-annotation. The authors also propose an efficient algorithm for fitting SMPL-family body models to the nonparametric point predictions. NLF outperforms prior work on a variety of benchmarks, including 3DPW, EMDB, EHF, SSP-3D, and AGORA.",
    "tags": [
      "3D Human Pose Estimation",
      "3D Human Shape Estimation",
      "Neural Fields",
      "Deep Learning",
      "Computer Vision"
    ]
  },
  {
    "title": "VGGHeads: A Large-Scale Synthetic Dataset for 3D\nHuman Heads",
    "description": "The document introduces VGGHeads, a large-scale synthetic dataset designed for advancing 3D human head modeling in computer vision. Generated using diffusion models, VGGHeads comprises over a million high-resolution images, each meticulously annotated with 3D head meshes, facial landmarks, and bounding boxes. This addresses the limitations of traditional datasets, which often suffer from bias, privacy concerns, and limited representation. The authors also present a novel model architecture trained on VGGHeads, capable of reconstructing detailed 3D head meshes from single images. Through extensive experiments, they demonstrate the dataset's effectiveness in various head-related tasks, including head pose estimation, 3D head alignment, and face detection, showcasing its potential to significantly advance the field of 3D human head modeling.",
    "tags": [
      "3D Head Model",
      "Synthetic Dataset",
      "Diffusion Models",
      "Computer Vision",
      "Head Pose Estimation"
    ]
  },
  {
    "title": "Tora: Trajectory-oriented Diffusion Transformer for Video Generation",
    "description": "Tora is a new video generation model that uses trajectories to control the motion of generated objects. It is the first model to integrate text, images, and trajectories into a single model, enabling scalable video generation with robust motion control. Tora is built on top of OpenSora, an open-source version of the Sora diffusion model, and introduces two novel modules: the Trajectory Extractor (TE) and the Motion-guidance Fuser (MGF). The TE converts arbitrary trajectories into hierarchical spacetime motion patches, and the MGF integrates these patches within the stacked DiT blocks. Tora can generate high-quality videos that adhere to specified trajectories, producing up to 204 frames at 720p resolution. This capability underscores Tora's versatility and robustness in handling diverse motion patterns while maintaining high visual fidelity.",
    "tags": [
      "Video Generation",
      "Diffusion Models",
      "Trajectory Control",
      "Motion Guidance",
      "Diffusion Transformer"
    ]
  }
]