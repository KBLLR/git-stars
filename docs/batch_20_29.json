[
  {
    "title": "A Statistical Explanation of the Dunning-Kruger Effect",
    "description": "This discussion paper presents a statistical explanation for the Dunning-Kruger effect, a cognitive bias where people with low ability tend to overestimate their ability. The authors argue that the effect is a statistical artifact due to boundary constraints in data, rather than a psychological phenomenon. They demonstrate this through a simple statistical model that accounts for these constraints and achieves a near-perfect fit to data from undergraduate statistics exams.",
    "tags": [
      "Dunning-Kruger effect",
      "Boundary conditions",
      "Tobit model",
      "Statistical artifact",
      "Overconfidence"
    ],
    "document": "documents/21092.pdf"
  },
  {
    "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice\\Conversion for everyone",
    "description": "This document presents YourTTS, a novel zero-shot multi-speaker and multilingual text-to-speech model. The authors build upon the VITS model and introduce several modifications to achieve state-of-the-art results in zero-shot multi-speaker TTS and voice conversion tasks. The model leverages raw text input, multilingual training, and speaker consistency loss for improved performance.  YourTTS demonstrates promising results in synthesizing speech for unseen speakers, even in low-resource languages with limited training data. The authors also explore speaker adaptation capabilities, showing that the model can be fine-tuned with less than a minute of speech to synthesize speech for speakers with unique voice characteristics. ",
    "tags": [
      "Text-to-Speech (TTS)",
      "Zero-Shot Multi-Speaker TTS",
      "Zero-Shot Voice Conversion",
      "Speaker Adaptation",
      "Multilingual TTS"
    ],
    "document": "documents/2112.02418v4 (1).pdf"
  },
  {
    "title": "YourTTS: Towards Zero-Shot Multi-Speaker TTS and Zero-Shot Voice\nConversion for everyone",
    "description": "This document presents YourTTS, a novel zero-shot multi-speaker and multilingual text-to-speech model. YourTTS builds upon the VITS model and introduces several modifications for zero-shot multi-speaker and multilingual training. The authors demonstrate state-of-the-art results in zero-shot multi-speaker TTS and results comparable to SOTA in zero-shot voice conversion on the VCTK dataset. Additionally, YourTTS achieves promising results in a target language with a single-speaker dataset, opening possibilities for zero-shot multi-speaker TTS and zero-shot voice conversion systems in low-resource languages. The model can be fine-tuned with less than 1 minute of speech to achieve state-of-the-art results in voice similarity and reasonable quality, enabling synthesis for speakers with diverse voices or recording characteristics.",
    "tags": [
      "Text-to-Speech (TTS)",
      "Zero-Shot Multi-Speaker TTS",
      "Zero-Shot Voice Conversion",
      "Speaker Adaptation",
      "Multilingual TTS"
    ],
    "document": "documents/2112.02418v4.pdf"
  },
  {
    "document": "documents/2205.11487v1.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "GRAM-HD: 3D-Consistent Image Generation at High Resolution with Generative Radiance Manifolds",
    "description": "This paper proposes GRAM-HD, a novel 3D-aware GAN method that synthesizes high-resolution images with strong 3D consistency. Existing methods either struggle to generate high-resolution images while maintaining 3D consistency or rely on 2D CNNs for upsampling, which compromises 3D consistency. GRAM-HD addresses this by performing super-resolution directly in the 3D space using a set of 2D radiance manifolds. This approach leverages the efficiency of 2D CNNs for upsampling while preserving 3D consistency through volume rendering. Experiments on FFHQ and AFHQv2 datasets demonstrate that GRAM-HD generates high-quality, 3D-consistent results outperforming existing methods in terms of both quality and speed.",
    "tags": [
      "3D-aware GAN",
      "Image Generation",
      "High Resolution",
      "3D Consistency",
      "Generative Radiance Manifolds"
    ],
    "document": "documents/2206.07255v2.pdf"
  },
  {
    "title": "PHENAKI: VARIABLE LENGTH VIDEO GENERATION\nFROM OPEN DOMAIN TEXTUAL DESCRIPTIONS",
    "description": "The document introduces Phenaki, a novel text-to-video model capable of generating videos of variable lengths from a series of text prompts, essentially narrating a visual story.  Key to Phenaki's innovation is the C-ViViT model, enabling efficient temporal-spatial video compression while maintaining an auto-regressive mechanism for dynamic video generation. Phenaki demonstrates superior performance in generating coherent and diverse videos, even with limited text-video training data, by leveraging the vastness of image-text datasets. The model's ability to extrapolate videos from a single frame or text prompt, coupled with its capacity for dynamic scene generation through time-variable prompts, positions it as a potential game-changer in creative visual storytelling.",
    "tags": [
      "Video Generation",
      "Text-to-Video",
      "Open Domain",
      "Variable Length Video",
      "C-ViViT"
    ],
    "document": "documents/2210.02399v1.pdf"
  },
  {
    "document": "documents/2211.09800v2.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  },
  {
    "title": "Imagen Editor and EditBench: Advancing and Evaluating Text-Guided Image Inpainting",
    "description": "The document introduces Imagen Editor, a new text-guided image inpainting model that builds upon Imagen and is fine-tuned for inpainting. Imagen Editor leverages object detection during training to improve alignment between generated images and text prompts. The authors also present EditBench, a new benchmark dataset for evaluating text-guided image inpainting. EditBench includes a variety of images, text prompts, and difficulty levels to assess the performance of these models. Through human evaluation, Imagen Editor is shown to outperform DALL-E 2 and Stable Diffusion in terms of text-image alignment and image quality.",
    "tags": [
      "Text-guided Image Inpainting",
      "Image Editing",
      "Diffusion Models",
      "Benchmarking",
      "Imagen Editor",
      "EditBench"
    ],
    "document": "documents/2212.06909v2.pdf"
  },
  {
    "title": "Knowledge Reasoning via Jointly Modeling Knowledge Graphs and\nSoft Rules",
    "description": "This document proposes a novel method called Iterlogic-E for knowledge graph completion (KGC). This method leverages the accuracy and interpretability of rule-based reasoning and the efficiency and scalability of embedding-based reasoning. It iteratively injects rules and learns representations to enhance knowledge graph embeddings (KGEs) and remove incorrect rule conclusions. The method models conclusion labels as 0-1 variables and uses a rule confidence regularizer to handle the uncertainty of soft logical rules. Experiments on FB15K and DB100K datasets show that Iterlogic-E outperforms state-of-the-art methods, demonstrating its effectiveness in knowledge reasoning.",
    "tags": [
      "Distributed representation",
      "Knowledge graph",
      "Link prediction",
      "Logical rule",
      "Knowledge reasoning"
    ],
    "document": "documents/2301.02781v1.pdf"
  },
  {
    "document": "documents/2301.04104v2.pdf",
    "error": "Exceeded retry limit",
    "title": "",
    "description": "",
    "tags": []
  }
]